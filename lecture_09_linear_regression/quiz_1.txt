Q Line of regression always passes through:
âœ“ Center of mass
âœ˜ Center of medians
âœ˜ Random point/No specific point is known
âœ˜ Center of smallest circle that contains all datapoints
âœ˜ Origin (0,0)

Q The slope of the Standard Deviation line compared to the slope of the Regression line is always:
âœ“ Larger (the line is steeper)
âœ˜ Smaller (the line is flatter)
âœ˜ Equal
âœ˜ Random, i.e. depending on dataset can be larger or smaller

Q Can NOT be used to measure the goodness of fit of linear regression:
âœ˜ R squared (R^2)
âœ“ Chi squared (ğ›˜^2)
âœ˜ Mean squared error (MSE)
âœ˜ Mean absolute error (MAE)
âœ˜ Root Mean Squared Error (RMSE)

Q Why do we need adjusted R squared?
âœ“ plain (not adjusted) R squared increases automatically as you add new independent variables to a regression equation even if they don't contribute any new explanatory power to the equation.
âœ˜ Adjusted R squared is more easy to interpret in the context of the model.
âœ˜ Adjusted R squared always decreases with the addition of new variables.
âœ˜ Adjusted R squared is more accurate than R squared when comparing two models of equal dimension (eual number of internal parameters).

Q You have a dataset that contains 3 points {(1,1),(2,4),(4,5)}. What is the equation of the regression line?
âœ“ y = 0.5 + 1.2 x
âœ˜ y = 1 + 1.5 x
âœ˜ y = 1 + 1.0 x
âœ˜ y = 1.5 + 0.5 x

Q You have a dataset that contains 3 points {(1,1),(2,4),(4,5)}. What is R squared?
âœ“ R2 = 0.8
âœ˜ R2 = 0.7
âœ˜ R2 = 0.9
âœ˜ R2 = 1.0

Q Linear regression can be performed in the form y = k x + b and y = k x by sklearn. To switch to the mode y = k x + b, we should use the parameter:
âœ“ fit_intercept=True
âœ˜ fit_intercept=False
âœ˜ fit_slope=True
âœ˜ fit_slope=False

Q Given information matrix X and vector of classes y such that X w â‰ˆ y, one can define parameters of regression w by
âœ“ w = (X^T X)^(-1) X^T y = X^+ y
âœ˜ w = (X^T X) X^T y = X^+ y
âœ˜ w = (X^T X)^(-1) y = X^+ y
âœ˜ w = X^(-1) y

Q Given the information matrix X and vector of parameters w, the result of multiplication X w always belongs to:
âœ“ Columns space of X
âœ˜ Rows space of X
âœ˜ Null space of X
âœ˜ Left null space of X

Q For any training set, it is true:
âœ“ Var[data] = Var[residues] + Var[regression]
âœ˜ Var[data] = Var[residues] - Var[regression]
âœ˜ Var[data] = Var[residues] / Var[regression]
âœ˜ Var[data] = Var[residues] * Var[regression]

Q Linear regression can be used to fit (feature and class transformations allowed):
âœ“ y = k x + b
âœ˜ y = cos(k1 x) + k2 sin(x)
âœ“ y = k1 x^2 + k2 x + k3
âœ“ y = log(k x)
âœ˜ y = log(k1 x) + k2 x

Q Regularization is used to:
âœ“ Avoid overfitting
âœ˜ Remove outliers
âœ˜ Avoid underfitting
âœ˜ Increase model complexity

Q You are training model M with internal parameters k1,k2,â€¦,kN. What type of regularization you should use if you want the biggest one max(k1,k2,â€¦,kN) to be as small as possible?
âœ˜ L0
âœ˜ L1
âœ“ L2
âœ˜ Cannot be done with regularization

Q You are training model M with internal parameters k1,k2,â€¦,kN. What type of regularization you should use if you want as many of them as possible to be equal exactly to zero?
âœ˜ L0
âœ“ L1
âœ˜ L2
âœ˜ Cannot be done with regularization
